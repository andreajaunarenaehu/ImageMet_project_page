<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ImageMet: A General Purpose Multimodal Database for Metaphor Understanding</title>
  <link rel="icon" type="image/x-icon" href="static/images/hitzicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><strong>ImageMet </strong>: A General Purpose Multimodal Database for Metaphor Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/AndreaJaunarena-ehu" target="_blank">Andrea Jaunarena</a>,</span>
                    <a href="https://github.com/ragerri" target="_blank">Rodrigo Agerri</a>,</span>
                    <a href="https://github.com/gazkune" target="_blank">Gorka Azkune</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">HiTZ Center - Ixa<br>University of the Basque Country UPV/EHU</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2406.09952" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper (momentuz imanolena)</span>
                      </a>
                    </span>

                  <!-- BiVLC -->
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/AndreaJaunarena-Cayetano/ImageMet" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>ImageMet Dataset</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/AndreaJaunarena-ehu/ImageMet" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Metaphors pose a significant challenge for Vision-Language Models, which must integrate figurative meaning across visual and textual modalities. Furthermore, existing multimodal metaphor datasets remain limited in size, linguistic diversity, and task flexibility, restricting the systematic evaluation of models’ capabilities to process figurative language. In this work, we present ImageMet, a synthetically generated and human-validated multimodal dataset, where each instance is composed of: i) a visual metaphor, ii) a figurative and literal textual counterpart, iii) the specifications of the explicit source-target mappings, and iv) the meaning of the generated linguistic metaphor. Grounded in conceptual metaphor theory, ImageMet enables a unified evaluation across four key dimensions: metaphor detection, interpretation, generation, and cross-modal mapping. Using ImageMet, in this paper, we benchmark state-of-the-art dual-encoder Vision-Language Models (VLMs) and Multimodal LLMs (MLLMs) on image-text retrieval, visual entailment, and metaphor generation. Results show that dual-encoder models struggle with figurative cross-modal alignment, MLLMs often rely on surface-level visual overlaps rather than meaning for entailment decisions, and metaphor generation improves substantially when models are visually grounded. These findings reveal persistent gaps in the multimodal figurative understanding research field and demonstrate the value of ImageMet for advancing research on metaphor processing across modalities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- BiVLC -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">ImageMet: A General Purpose Multimodal Database for Metaphor Understanding</h2>
      <h2 class="title is-2">What is ImageMet?</h2>
      <div class="level-set has-text-justified">
        <p>
          <strong>ImageMet </strong> is a general-purpose multimodal dataset for metaphor understanding, designed to evaluate how Vision Language Models (VLMs)
          and Multimodal Large Language Models (MLLMs) process figurative meaning across modalities (images and text).
        </p>
        <img style="width: 100%; max-width: 1500px; height: auto;" src="static/introduction_figure-2.jpg" alt="MY ALT TEXT"/>
       <p> 
         Unlike previous datasets, ImageMet supports multiple metaphor-related tasks, including metaphor detection, interpretation, generation
         and cross-modal mapping. 
       </p>
     </div> 
      <br>
      <h2 class="title is-2">Why ImageMet?</h2>
      <div class="level-set has-text-justified">
        <p>
          Metaphors are central for human communication, but they remain challenging for multimodal models. In addition, existing 
          multimodal metaphor datasets suffer from key limitations:
          <ol class="ml-6">
            <li>Redundancy in both textual and visual components.</li>
            <li>Very short linguistic metaphors (typically fewer that 5 words).</li>
            <li>A dataset size that becomes insufficient when adapted to tasks beyond explainable visual entailment.</li>
            <li>Lack of publicly available data.</li>
          </ol>
        </p>
        <br>
        <p>
          As a result, systematic evaluation of multimodal metaphor understanding is still missing. 
          To address this gap, ImageMet: 
          <ol class="ml-6">
            <li>Supports evaluation across multiple tasks, without varying the dataset size.</li>
            <li>Includes linguistic metaphors grounded in established theoretical frameworks, such as conceptual metaphor theory
              as proposed by Lakoff and Johnson (1980).</li>
            <li>Has 639 unique textual and visual representations.</li>
            <li>Includes metaphorical expressions of varied length and linguistic complexity.</li>
            <li>Contains literal textual counterparts for each metaphor, which capture the intended meaning of the metaphor. That way, 
            we can test models whether they comprehend metaphorical meaning or just rely on surface-level formal patterns.</li>
          </ol>
        </p>
      </div>
       <br> 
      <h2 class="title is-2">What does an ImageMet instance contain?</h2>
      <div class="level-set has-text-justified">
      <p>
        <img style="width: 100%; max-width: 1500px; height: auto;" src="static/Examples_web_orria_40_2.jpg" alt="MY ALT TEXT"/>
        <strong>ImageMet </strong> has 639 instances consisting of: 
      </p>
        <ol class="ml-6">
          <li>a <strong>visual metaphor </strong> (image),</li>
          <li>the <strong>source </strong> and the <strong>target </strong> of the conceptual mapping,</li>
          <li>the <strong>generated linguistic metaphor </strong>,</li>
          <li>its contradicting metaphor (<strong>contradicting metaphor </strong>),</li>
          <li>the <strong>entailing literal </strong> (which expresses the same idea of the linguistic metaphor without using figurative language),</li>
          <li>the <strong>contradicting literal </strong> (which expresses an idea that contradicts or is opposite to the original metaphor's meaning),</li>
          <li>a description of the meaning of the generated linguistic metaphor (the concept or idea that the linguistic metaphor conveys in a short sentence) (<strong>literal description </strong>),</li>
          <li>the <strong>objects </strong>, <strong>properties </strong>, and <strong>relations </strong> that appear in the visual metaphor, and</li>
          <li>the <strong>visual elaboration </strong> used to create the visual metaphor.</li>
        </ol> 
     </div> 
      <br>
      <h2 class="title is-2">How was ImageMet built?</h2>
      <div class="level-set has-text-justified">
        <p>
          <img style="width: 100%; max-width: 1500px; height: auto;" src="static/DATASET_ESQUEMA(1).png" alt="MY ALT TEXT"/>
          ImageMet was created using a semi-automated, human-validated pipeline grounded in linguistic theory:
          <ol class="ml-6">
          <li><strong>Conceptual mappings </strong> collected from the Master Metaphor list.</li>
          <li><strong>Text generation</strong> (generated linguistic metaphor, contradicting metaphor, entailing literal and contradicting literal) prompting
          Claude. </li>
          <li><strong>Visual elaborations </strong> creation from generated linguistic metaphors.</li>
          <li><strong>Visual metaphors </strong> from visual elaborations.</li>
          <li><strong>Humman annotation </strong>  and filtering by expert annotators.</li>
        </ol>
        <br>
        After validations, the dataset contains 639 high-quality multimodal metaphor instances.
        </p>
      </div>
      <br>
      <h2 class="title is-2">What tasks does ImageMet support?</h2>
      <div class="level-set has-text-justified">
        ImageMet enables evaluation across four key dimensions of metaphor understanding: (1) metaphor detection, (2) metaphor generation,
        (3) metaphor interpretation, and (4) cross-modal mapping. 
        <br>
        However, in this paper, three representative tasks are benchmarked: (i) image-text retrieval, (ii) visual entailment, and (iii) metaphor generation. 
      </div>
      <br>
      <h2 class="title is-2">Main findings</h2>
      <div class="level-set has-text-justified">
        <p>Experiments on state-of-the-art VLMs and MLLMs reveal clear limitations:</p>
         <ol class="ml-6">
           <li><strong>Image-text retrieval:</strong></li>
             <ol class="ml-6">
               <li>Retrieval tasks become more challenging in the presence of metaphorical instances.</li>
               <li>Models performing best on traditional multimodal datasets (e.g, COCO) 
                 tend to achieve the lowest scores on equivalent tasks involving metaphorical language and image</li>
               <li> VLMs cross-modal matching between visual metaphors and their counterpart textual instances rely
                 more heavily on form-based cues than on semantic meaning.</li>
             </ol>
           <li><strong>Visual entailment:</strong></li>
             <ol class="ml-6">
               <li>Models obtain higher accuracy on figurative hypotheses than on literal ones, 
               suggesting they focus on surface-level overlap of visual elements between
              images and figurative texts instead of the semantic meaning of the visual entailment components. </li>
             </ol>
           <li><strong>Metaphor generation:</strong></li>
             <ol class="ml-6">
               <li>MLLMs generate significantly better metaphors when visually grounded (supported with a visual metaphor).</li>
               <li>Visual metaphors act as a conceptual scaffold, improving creativity and coherence.</li>
             </ol>
         </ol>
      </div>
      </p></div>
   </div>
</section>
<!-- End BiVLC -->

<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 style="text-align: center;" class="title is-3">Instances from ImageMet dataset</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img style="width: 100%; max-width: 1500px; height: auto;" src="static/Examples_web_orria_0_0.jpg" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img style="width: 100%; max-width: 1500px; height: auto;" src="static/Examples_web_orria_1018_1.jpg" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img style="width: 100%; max-width: 1500px; height: auto;" src="static/Examples_web_orria_10_0.jpg" alt="MY ALT TEXT"/>
     </div>
     <div class="item">
        <!-- Your image here -->
        <img style="width: 100%; max-width: 1500px; height: auto;" src="static/Examples_web_orria_40_2.jpg" alt="MY ALT TEXT"/>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ImageMet: A General Purpose Multimodal Database for Metaphor Understanding</title>
  <link rel="icon" type="image/x-icon" href="static/images/hitzicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><strong>ImageMet </strong>: A General Purpose Multimodal Database for Metaphor Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/AndreaJaunarena-ehu" target="_blank">Andrea Jaunarena</a>,</span>
                    <a href="https://github.com/ragerri" target="_blank">Rodrigo Agerri</a>,</span>
                    <a href="https://github.com/gazkune" target="_blank">Gorka Azkune</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">HiTZ Center - Ixa<br>University of the Basque Country UPV/EHU</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2406.09952" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper (momentuz imanolena)</span>
                      </a>
                    </span>

                  <!-- BiVLC -->
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/AndreaJaunarena-Cayetano/ImageMet" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>ImageMet Dataset</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/AndreaJaunarena-ehu/ImageMet" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Metaphors pose a significant challenge for Vision-Language Models, which must integrate figurative meaning across visual and textual modalities. Furthermore, existing multimodal metaphor datasets remain limited in size, linguistic diversity, and task flexibility, restricting the systematic evaluation of models’ capabilities to process figurative language. In this work, we present ImageMet, a synthetically generated and human-validated multimodal dataset, where each instance is composed of: i) a visual metaphor, ii) a figurative and literal textual counterpart, iii) the specifications of the explicit source-target mappings, and iv) the meaning of the generated linguistic metaphor. Grounded in conceptual metaphor theory, ImageMet enables a unified evaluation across four key dimensions: metaphor detection, interpretation, generation, and cross-modal mapping. Using ImageMet, in this paper, we benchmark state-of-the-art dual-encoder Vision-Language Models (VLMs) and Multimodal LLMs (MLLMs) on image-text retrieval, visual entailment, and metaphor generation. Results show that dual-encoder models struggle with figurative cross-modal alignment, MLLMs often rely on surface-level visual overlaps rather than meaning for entailment decisions, and metaphor generation improves substantially when models are visually grounded. These findings reveal persistent gaps in the multimodal figurative understanding research field and demonstrate the value of ImageMet for advancing research on metaphor processing across modalities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- BiVLC -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">ImageMet: A General Purpose Multimodal Database for Metaphor Understanding</h2>
      <div class="level-set has-text-justified">
        <p>
          <strong>ImageMet </strong> is a synthetically generated and human-validated dataset that was constructed using a systematic methodology grounded in the source-target domain mappings established by Lakoff and Johnson (1980) and on the linguistic metaphor annotation guidelines developed by Pragglejaz (2007); Steen et al. (2010); Sanchez-Bayona and Agerri (2022).
        </p>
       <p> 
       </p>
     </div> 
       <br> <!-- Line break added here -->
      <div class="level-set has-text-justified">
      <p>
        <strong>ImageMet </strong> has 639 instances consisting of: a <strong>visual metaphor </strong> (image), the <strong>source </strong> and the <strong>target </strong>
        of the conceptual mapping, the <strong>generated linguistic metaphor </strong>, its contradicting metaphor (<strong>contradicting metaphor </strong>),
        the <strong>entailing literal </strong> (which expresses the same idea of the linguistic metaphor without using figurative language), 
        the <strong>contradicting literal </strong> (which expresses an idea that contradicts or is opposite to the original metaphor's meaning),
        a description of the meaning of the generated linguistic metaphor (the concept or idea that the linguistic metaphor conveys in a short sentence) (<strong>literal description </strong>),
        the <strong>objects </strong>, <strong>properties </strong>, and <strong>relations </strong> that appear in the visual metaphor, and 
        the <strong>visual elaboration </strong> used to create the visual metaphor.
       </p> 
     </div>  
      </p></div>
   </div>
</section>
<!-- End BiVLC -->

<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 style = "margin-left: 400px" class="title is-3">Instances from ImageMet dataset</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img width="800" style = "margin-left: 250px" src="static/Examples_web_orria_0_0.jpg" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img width="800" style = "margin-left:  250px" src="static/Examples_web_orria_1018_1.jpg" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img width="800" style = "margin-left:  250px" src="static/Examples_web_orria_10_0.jpg" alt="MY ALT TEXT"/>
     </div>
     <div class="item">
        <!-- Your image here -->
        <img width="800" style = "margin-left:  250px" src="static/Examples_web_orria_40_2.jpg" alt="MY ALT TEXT"/>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
